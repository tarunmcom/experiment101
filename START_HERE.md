# ðŸš€ START HERE - U-Net Document Tampering Detection

Welcome! This guide will get you started in **3 simple steps**.

---

## ðŸ“‹ What You Have

A complete PyTorch implementation for detecting tampered regions in documents using U-Net:

âœ… **Advanced Loss Function** - Handles small masks and class imbalance  
âœ… **U-Net Architecture** - ~31M parameters, encoder-decoder with skip connections  
âœ… **Training Best Practices** - LR scheduling, early stopping, checkpointing  
âœ… **Comprehensive Metrics** - IoU, Dice, Precision, Recall, F1  
âœ… **Visualization Tools** - Sample predictions, training curves, evaluation plots  
âœ… **Complete Documentation** - 5 guides, 7 scripts, ready to use  

---

## ðŸŽ¯ Quick Start (3 Steps)

### Step 1: Install Dependencies (2 minutes)

```bash
pip install -r requirements.txt
```

### Step 2: Verify Setup (1 minute)

```bash
python test_setup.py
```

This checks:
- âœ“ All packages installed
- âœ“ CUDA availability (GPU support)
- âœ“ Dataset accessibility
- âœ“ Model creation

### Step 3: Train Model (1-2 hours)

```bash
python train_unet.py
```

**That's it!** The model will train automatically with all best practices.

---

## ðŸ“Š What Happens During Training?

```
Training Progress:
Epoch 10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| loss: 0.1234, iou: 0.8567, dice: 0.8901
Epoch 10 [Val]:   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| loss: 0.1456, iou: 0.8234, dice: 0.8567

âœ“ Saved best model (IoU: 0.8234)
```

**Automatic Features:**
- Saves best models based on IoU and loss
- Generates sample predictions every 5 epochs
- Reduces learning rate when validation plateaus
- Stops early if no improvement (15 epochs patience)
- Creates training history plots

**Output Files:**
```
checkpoints/
â”œâ”€â”€ best_model_iou.pth     â­ Use this for inference
â”œâ”€â”€ best_model_loss.pth
â””â”€â”€ final_model.pth

predictions/
â”œâ”€â”€ epoch_5.png
â”œâ”€â”€ epoch_10.png
â””â”€â”€ ...

training_history.png        ðŸ“ˆ Loss and metrics over time
```

---

## ðŸ” After Training

### Option A: Test on Samples

```bash
python inference.py --num_samples 10
```

Shows predictions with metrics for 10 test images.

### Option B: Comprehensive Evaluation

```bash
python evaluate.py
```

Generates:
- Per-image and global metrics
- ROC and Precision-Recall curves  
- Confusion matrix
- Metric distributions

---

## ðŸ“š Documentation

| File | Purpose | When to Read |
|------|---------|--------------|
| **START_HERE.md** | This file - Quick start | First! |
| **QUICKSTART.md** | Step-by-step guide | Getting started |
| **README.md** | Complete documentation | Deep dive |
| **WORKFLOW.md** | Visual workflow diagrams | Understanding process |
| **PROJECT_SUMMARY.md** | Technical overview | Implementation details |

---

## ðŸŽ“ Key Features Explained

### Why Combined Loss?

Your dataset has **challenging characteristics**:
1. âš ï¸ Small masks vs. large backgrounds (class imbalance)
2. âš ï¸ Variable mask sizes (tiny to large)
3. âš ï¸ Multiple disconnected masks

**Solution:** Combined Loss = 0.5Ã—BCE + 0.3Ã—Dice + 0.2Ã—Focal

- **BCE**: Standard loss, stable gradients
- **Dice**: Focuses on overlap, handles imbalance
- **Focal**: Emphasizes hard examples, ignores easy background

This combination ensures robust learning across all mask sizes! âœ¨

### What is U-Net?

```
    Input Image (512Ã—512Ã—3)
           â†“
    [Encoder: Extract features]
           â†“
    [Bottleneck: Deepest features]
           â†“
    [Decoder: Reconstruct mask]
     + Skip connections
           â†“
    Output Mask (512Ã—512Ã—1)
```

U-Net is perfect for segmentation because:
- âœ“ Preserves spatial information (skip connections)
- âœ“ Captures both context and detail
- âœ“ Works well with limited data

---

## âš™ï¸ Configuration (Optional)

Default settings work well, but you can customize in `train_unet.py`:

```python
config = {
    'batch_size': 8,           # Reduce if GPU memory issue
    'num_epochs': 100,         # Max training epochs
    'learning_rate': 1e-3,     # Initial learning rate
    'early_stopping_patience': 15,  # Epochs to wait
}
```

---

## ðŸŽ¯ Expected Results

After training (~50 epochs), you should see:

| Metric | Expected Value |
|--------|---------------|
| Training IoU | 0.85 - 0.95 â­ |
| Validation IoU | 0.75 - 0.88 â­ |
| Dice Coefficient | 0.80 - 0.92 â­ |
| Precision | 0.80 - 0.95 |
| Recall | 0.75 - 0.90 |

**Interpretation:**
- IoU > 0.80 = Very good overlap âœ…
- IoU 0.70-0.80 = Good âœ“
- IoU < 0.70 = Needs improvement âš ï¸

---

## ðŸ› Troubleshooting

### Problem: Out of Memory

```python
# In train_unet.py, reduce batch size:
config['batch_size'] = 4  # or even 2
```

### Problem: No GPU / CUDA not available

**Don't worry!** The code automatically uses CPU. It's slower but works.

### Problem: Model not learning

1. Run `python check_dataset.py` to verify data
2. Check that masks are binary (0 and 255)
3. Try lower learning rate: `config['learning_rate'] = 1e-4`

### Problem: Dataset not found

Ensure folder structure:
```
.
â”œâ”€â”€ DocTamperV1-TrainingSet/
â”‚   â”œâ”€â”€ data.mdb
â”‚   â””â”€â”€ lock.mdb
â””â”€â”€ DocTamperV1-TestingSet/
    â”œâ”€â”€ data.mdb
    â””â”€â”€ lock.mdb
```

---

## ðŸŽ‰ Pro Tips

1. **Always check dataset first**: `python check_dataset.py`
2. **Monitor training**: Watch `predictions/` folder for visual progress
3. **Use best_model_iou.pth**: This is your best model for inference
4. **Compare results**: Use `evaluate.py` for detailed analysis
5. **Save time**: Early stopping prevents unnecessary training

---

## ðŸ“– Complete File List

### Scripts (Run these)
- `test_setup.py` - Verify installation
- `check_dataset.py` - Verify dataset
- `train_unet.py` - **Main training script** â­
- `inference.py` - Test trained model
- `evaluate.py` - Comprehensive evaluation

### Documentation (Read these)
- `START_HERE.md` - This file
- `QUICKSTART.md` - Quick guide
- `README.md` - Full documentation
- `WORKFLOW.md` - Visual workflows
- `PROJECT_SUMMARY.md` - Technical details

### Dependencies
- `requirements.txt` - Python packages

### Original
- `vizlmdb.py` - LMDB viewer (from dataset)

---

## âš¡ Ultra-Quick Command Sequence

```bash
# If you're in a hurry, just run these:
pip install -r requirements.txt
python test_setup.py
python train_unet.py

# Wait 1-2 hours...

python inference.py --num_samples 10
# Done! âœ…
```

---

## ðŸ¤” Need Help?

### Quick Questions
- "How long does training take?" â†’ 1-2 hours with GPU, 10-20 hours with CPU
- "Which model file to use?" â†’ `checkpoints/best_model_iou.pth`
- "How to test on one image?" â†’ `python inference.py --index 1`
- "How to change batch size?" â†’ Edit `config['batch_size']` in `train_unet.py`

### Detailed Questions
- Check `README.md` for complete documentation
- Check `QUICKSTART.md` for step-by-step guide
- Check `PROJECT_SUMMARY.md` for technical details

---

## âœ… Checklist

Before training:
- [ ] Installed dependencies (`pip install -r requirements.txt`)
- [ ] Verified setup (`python test_setup.py`)
- [ ] Checked dataset (`python check_dataset.py`)

During training:
- [ ] Monitor progress in terminal
- [ ] Check `predictions/` folder periodically
- [ ] Verify checkpoints are being saved

After training:
- [ ] Run evaluation (`python evaluate.py`)
- [ ] Test on samples (`python inference.py`)
- [ ] Review metrics and visualizations

---

## ðŸŽŠ Ready to Start!

You now have everything you need. Just run:

```bash
python train_unet.py
```

The training will handle everything automatically. Good luck! ðŸš€

---

**Next Steps:**
1. Run the training
2. Monitor the progress
3. Evaluate the results
4. Test on your images

**Questions?** Check the documentation files listed above.

**Happy Training!** ðŸŽ¯

